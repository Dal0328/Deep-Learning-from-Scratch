{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a845aa",
   "metadata": {},
   "source": [
    "# 4. Training NN\n",
    "\n",
    "## 4.2 Loss Function\n",
    "손실함수의 미분값을 바탕으로 학습을 한다. 정확도를 지표로 사용하지 않는 이유는, 대부분의 미분값이 0이기 때문이다. 이는 정확도가 불연속적인 값이기 때문이다. 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않는다.\n",
    "\n",
    "### 4.2.1 Sum of Squares for error(SSE)\n",
    "\n",
    "$$E = \\frac{1}{2}\\displaystyle\\sum_{k}^{}{(y_k-t_k)^2}$$\n",
    "\n",
    "### 4.2.2 Cross Entropy error(CEE)\n",
    "\n",
    "$$E = -\\displaystyle\\sum_{k}^{}{t_klogy_k}$$\n",
    "\n",
    "### 4.2.3 미니 배치 학습\n",
    "\n",
    "$$E = -\\frac{1}{N}\\displaystyle\\sum_{n}^{}\\displaystyle\\sum_{k}^{}{t_{nk}logy_{nk}}$$\n",
    "\n",
    "하지만, 데이터가 많을 경우, 모든 데이터에 대하여 손실함수를 계산하는 것은 비효율적이다. 이런 경우 데이터 일부를 추려 전체의 근사치로 이용한다. 이 일부를 미니 배치라고 하고, 이러한 학습 방법을 미니 배치 학습이라고 한다.\n",
    "\n",
    "정답이 원-핫 인코딩이 아니라 레이블로 표현되어 있을 경우 다음과 같이 손실 함수를 계산할 수 있다.\n",
    "\n",
    "```python\n",
    "return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "```\n",
    "\n",
    "## 4.4 Gradient\n",
    "편미분을 벡터로 정리한 것을 기울기라 한다. 함수가 극소, 극대, 안장점(관점에 따라 극대, 극소가 될 수 있는 지점)이 되는 장소에서는 기울기가 0이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중앙 차분을 이용한 수치 미분. 함수를 매개변수로 넣는다.\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4                    # 너무 작은 값을 넣으면 반올림 오차 발생\n",
    "    grad = np.zeros_like(x)     # x와 형상이 같은 배열 생성\n",
    "    \n",
    "    for idx in range(x.size):  # 각 매개변수별로 편미분 수행\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d85e7",
   "metadata": {},
   "source": [
    "### 4.4.1 Gradient Descent\n",
    "기울기가 가리키는 곳으로 일정 거리만큼 이동한다. 해당 과정을 일정 횟수 반복한다.\n",
    "- 학습률($\\eta$): 하이퍼파라미터\n",
    "\n",
    "$$x_0 = x_0 - \\eta\\frac{df}{dx_0}$$\n",
    "\n",
    "## 4.5 Stochastic Gradient Descent(SGD)\n",
    "1. 미니배치: 훈련 데이터 중 일부를 무작위로 가져온다.\n",
    "2. 기울기 산출: 가중치 매개변수의 기울기를 계산한다.\n",
    "3. 매개변수 갱신\n",
    "4. 반복\n",
    "5. Overfitting이 발생하기 전에 조기 종료(early stopping): test 데이터셋에 대한 정확도가 감소할 때 train 데이터셋에 대한 정확도가 높다면 조기 종료한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
